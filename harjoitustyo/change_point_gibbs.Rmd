---
title: "Bayesian change point detection"
author: "Ville Laitinen"
date: Sys.Date()
output:
  html_document:
    code_folding: "hide"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(magrittr)
library(tidyverse)
library(cowplot)
library(reshape2)
theme_set(theme_bw(20))

# Set seed for reproducibility
set.seed(112358)

```

# Model

In this report I investige a counting process which experiences a change point at an unknown location in time. The objective is to implement a Gibbs sampler that learns the location of this change point. Moreover, the other model parameters, the counting process rates $\mu_i$ are learned. We assume the data $\{y\}_{i=1}^n$ is generated by the process

$$y_i | \mu_1, \mu_2, \lambda \sim \begin{cases} 
      \text{Poisson}(\mu_1), & \text{if } i \leq \lambda \\
      \text{Poisson}(\mu_2), & \text{if } i > \lambda \\
      \end{cases}$$
where $\lambda$ denotes the change point. We assume conjugate priors for the parameters, namely

$$ \begin{align*}
\mu_1  &\sim \text{Gamma}(\alpha_1, \beta_1) \\
\mu_2  &\sim \text{Gamma}(\alpha_2, \beta_2) \\
\lambda &\sim \text{Uniform}(1, 2, \ldots, n)
\end{align*} $$

and hyperparameter values $\alpha_i = 2, \, \beta_i = 0.25$ so that the priors hold little information and the inference is mainly determined by data. 

```{r}

# Set hyperparameters
alpha <- c(2, 2)
beta <- c(0.25, 0.25)

```


## Likelihood 

As we assume our data to be identically and independently distributed (given the change point $\lambda$) the likelihood function is the product of likelihoods of individual observations: 

$$
\begin{align*}
  \text{Likelihood} &= \prod_{i = 1}^{\lambda}\text{Poisson}(y_i | \mu_1) \prod_{i = \lambda + 1}^{n}\text{Poisson}(y_i | \mu_2) \\
      & = \prod_{i = 1}^{\lambda} \frac{\mu_{1}^{y_i} e^{-\mu_1}}{y_i!} 
      \prod_{i = \lambda + 1}^{n} \frac{\mu_{2}^{y_i} e^{-\mu_2}}{y_i!} \\ 
      & = \mu_{1}^{\sum_{i = 1}^{\lambda} y_i} \mu_{2}^{\sum_{i = \lambda + 1}^{n} y_i}e^{-\lambda\mu_1 - (n-\lambda)\mu_2}\frac{1}{\prod_{i = 1}^{n}y_i!}.
            
\end{align*}
$$

The likelihood function in log scale is easily implemented in R. 

```{r}

LL <- function(data, mu, lambda) {
  
  N <- length(data)
  
  
  ll <- log(mu[1])*(sum(data[1:lambda])) + 
    log(mu[2])*(sum(data[(lambda+1):N])) -
    lambda*mu[1] - (N-lambda)*mu[2]
  
  return(ll)
  
}


```

## joint posterior density

Posterior distribution without normalizing factor is obtained by taking the product of likelihood and priors:

$$
\begin{align*}
\text{Posterior} &\propto \text{Likelihood} \cdot \text{Priors} \\
                 & = \text{Likelihood} \cdot \text{Gamma}(\alpha_1, \beta_1) \cdot \text{Gamma}(\alpha_2, \beta_2) \cdot \text{Unif}(1, 2, \cdots, n) \\
                 & = \text{Likelihood} \cdot \prod_{i = 1}^2 \frac{\beta_i^{\alpha_i}}{\Gamma(\alpha_i)}e^{-\beta_i\mu_i} \cdot \frac{1}{n} \\
                 & \propto \mu_1^{\sum_{i=1}^{\lambda}y_i + \alpha_1 - 1} \cdot \mu_1^{\sum_{i=\lambda + 1}^{n}y_i + \alpha_2 - 1} \cdot e^{-(\lambda + \beta_1)\mu_1 - (n- \lambda + \beta_2)\mu_2}
\end{align*}
$$

From this expression it is evident that the selected priors are infact conjugate to Poisson likelihood.

It is now straight forward to get the conditional posterior densities for the parameters: 

$$p(\lambda | \mu_1, \mu_2) \propto \mu_1^{\sum_{i=1}^{\lambda}y_i} \cdot \mu_2^{\sum_{i=\lambda + 1}^{n}y_i} \cdot e^{(\mu_2 - 
\mu_1)\lambda} $$

$$
\begin{align*}
p(\mu_1 | \lambda, \mu_2) &\propto \mu_1^{\sum_{i=1}^{\lambda}y_i + \alpha_1 - 1} \cdot e^{-(\lambda + \beta_1)\mu_1} \\
                          & \text{Gamma}(\mu_1 | \sum_{i = 1}^{\lambda}y_i + \alpha_1, \lambda + \beta_1)
\end{align*}
$$
$$
\begin{align*}
p(\mu_2 | \lambda, \mu_1) &\propto \mu_2^{\sum_{i=\lambda + 1}^{n}y_i + \alpha_2 - 1} \cdot e^{-(n - \lambda + \beta_2)\mu_2} \\
\end{align*}
$$



* The following functions sample from full conditional densities

```{r}
## mu1
r_mu1_cond <- function(data, mu2, lambda, alpha, beta) {
  
  shape <- sum(data[1:lambda]) + alpha[1] 
  rate <- lambda + beta[1]
  
  rgamma(1, shape = shape, rate = rate)

}

## mu2
r_mu2_cond <- function(data, mu1, lambda, alpha, beta) {
  
  N <- length(data)
  
  # if lambda == last time points, no change point exists
  # in this case return a sample from prior
  if(lambda == N) {
    return(rgamma(1, alpha[2], beta[2]))
  }
  
  shape <- sum(data[(lambda+1):length(data)]) + alpha[2] 
  rate <- N - lambda + beta[2]
  
  rgamma(1, shape = shape, rate = rate)
}

## lambda
# Implemented in log scale to avoid overflow
r_lambda_cond <- function(data, mu, alpha, beta) {

  N <- length(data)
  
  # mu1^(...)mu2^(...)
  c_log_mu <- sapply(1:N, FUN = function(i)  {
    
    (sum(data[1:i]) + alpha[1] - 1)*log(mu[1]) + ifelse(i == N, 0, (sum(data[(i+1):N]) + alpha[2] - 1)*log(mu[2]))
    
  }
  )
  
  # increment with exp term
  unnormalized_log_density <- c_log_mu -
    (beta[1]*mu[1] + N*mu[2] + beta[2]*mu[2]) +
    (mu[2] - mu[1])*(1:N)  
  
  
  # Rescale and exp transform to true scale
  density <- (unnormalized_log_density - 
    max(unnormalized_log_density) )%>% 
    exp
  
  # Normalize
  density <- density/sum(density)
  
  # Cumulative density
  cdf <- density %>% cumsum
  
  # Sample
  min(which(runif(1) <= cdf))
  
}

```


<!-- ```{r, eval = FALSE} -->
<!-- max_lambda_cond <- function(data, mu, alpha, beta) { -->

<!--   N <- length(data) -->


<!--   # Implement in log scale to avoid overflow -->
<!--   c_log_mu <- sapply(1:N, FUN = function(i)  { -->

<!--     (sum(data[1:i]) + alpha[1] - 1)*log(mu[1]) + ifelse(i == N, 0, (sum(data[(i+1):N]) + alpha[2] - 1)*log(mu[2])) -->

<!--   } -->
<!--   ) -->



<!--   unnormalized_log_density <- c_log_mu - -->
<!--     (beta[1]*mu[1] + N*mu[2] + beta[2]*mu[2]) + -->
<!--     (mu[2] - mu[1])*(1:N)   -->

<!--   log_density <- unnormalized_log_density/sum(unnormalized_log_density) -->

<!--   which.max(log_density) -->



<!-- } -->



<!-- xx <- lapply(seq(from = 1, to = 100, length.out = 100), function(i) { -->
<!--   lapply(seq(from = 1, to = 100, length.out = 100), function(j) { -->
<!--     max_lambda_cond(data, c(i, j), alpha, beta) -->
<!--     }) %>% unlist %>% cbind(j = 1:100, i = i) -->
<!--   }) %>% do.call(rbind, .) -->


<!-- xx <- xx %>% set_colnames(c("max_lambda", "mu2", "mu1")) -->


<!-- p <- xx %>% as.data.frame() %>% ggplot(aes(x = mu1, y = mu2, fill = max_lambda)) + geom_tile() -->

<!-- p -->
<!-- print(p) -->

<!-- ``` -->



## Gibbs sampler

We use sequential Gibbs sampling to draw samples from the posterior.

* The Gibbs sampler function. The parameter ```burn_in``` controls burn-in and is by default 25%. If a large amount of samples ```n``` are taken it may be convenient to keep only some of them to for example make visualization easier. The ```keep``` parameter controls the proportion of samples discarded. 
 
```{r}

gibbs <- function(data, alpha, beta, inits = "random", n, burn_in = 0.25, keep = 1) {
  
  # Random initialization unless supplied in the function call
  if(class(inits) == "character" & inits == "random") {
    inits <- c(runif(2, 1, max(data)), runif(1, 1, N))
  } 
  
  # Matrix for samples
  samples <- matrix(NA, n, 3) %>% 
    set_colnames(c("lambda", "mu1", "mu2"))
    
  # Set initial values
  samples[1, ] <- inits
  
  # Sequential gibbs sampling
  for(i in 2:n) {
    samples[i, "lambda"] <- r_lambda_cond(data = data,
                                          mu = samples[i-1, c("mu1", "mu2")],
                                          alpha = alpha,
                                          beta = beta)
    
    samples[i, "mu1"] <- r_mu1_cond(data = data,
                                    mu2 = samples[i-1, "mu2"],
                                    lambda = samples[i, "lambda"],
                                    alpha = alpha,
                                    beta = beta)
    
    samples[i, "mu2"] <- r_mu2_cond(data = data,
                                    mu1 = samples[i, "mu1"],
                                    lambda = samples[i, "lambda"],
                                    alpha = alpha,
                                    beta = beta)
  }
  
  # Remove burn-in
  samples <- samples[(round(nrow(samples)*burn_in) + 1):n, ]
  
  # thin
  samples <- samples[sample(1:nrow(samples), size = round(keep*nrow(samples))), ]
  
  samples <- samples %>% as.data.frame()
  
  return(samples)
  
}

```


## Model validation: simulations

Let us simulate data from the model and test how well the Gibbs sampler is able to recover true parameter values. Below the red vertical line denotes the change point and the blue lines the levels of Poisson process rates. 

```{r}

# Time series length
N <- 100

# Set hyperparameters
alpha <- c(2, 2)
beta <- c(.25, .25)

# Set parameters
lambda <- 75
mu <- c(5, 10)

# Simulate data
sim_data <- c(rpois(lambda, mu[1]), rpois(N - lambda, mu[2]))

# Plot
p_sim_example <- data.frame(y = sim_data, x = 1:N) %>%
  ggplot(aes(x = x, y = y)) +
  geom_vline(xintercept = lambda, color = "red", linetype = "dashed") + 
  geom_line(data = data.frame(x = c(0, lambda), y = c(mu[1], mu[1])), aes(x = x, y = y), color = "blue", linetype = "dashed", size = 1.5) + 
  geom_line(data = data.frame(x = c(lambda, N), y = c(mu[2], mu[2])), aes(x = x, y = y), color = "blue", linetype = "dashed", size = 1.5) + geom_point() 
  
print(p_sim_example)

```



Next, we make 10000 iterations and discard the first 50% as burn-in. Initial values are sampled from prior densities. 


```{r}
sim_samples <- gibbs(data = sim_data, 
      alpha = alpha, 
      beta = beta,
      inits = "random",
      n = 10000,
      burn_in = 0.5, 
      keep = 1)

```


The estimates have significant mass around the true values (in red)

```{r out.width=c('50%', '50%'), fig.show='hold', echo = FALSE}

p_sim_lambda <- sim_samples %>% 
  ggplot(aes(x = lambda)) + 
  stat_density(geom = "line") + 
  geom_vline(xintercept = lambda, linetype = "dashed", color = "red") + 
  labs(x = "Value", y = "Density", title = expression(lambda ~" posterior"))

p_sim_mu <- sim_samples %>% 
    ggplot(aes(x = mu1, y = mu2)) + 
    geom_hex(aes(fill=..density..)) + 
    geom_point(aes(x = mu[1], y = mu[2]), color = "red", size = 3) + 
    labs(title = expression(mu~" posterior"), x = expression(mu["1"]), y = expression(mu["2"])) + 
  guides(fill = guide_legend("Density"))

# p_sim_posterior <- plot_grid(p_sim_lambda, p_sim_mu, nrow = 2)

p_sim_lambda
p_sim_mu

```




# 4. Real data

Let us know test our Bayesian change point detector on data with unknown parameters.

* Data
```{r}

data <- c(4, 4, 3, 1, 3, 2, 1, 0, 11, 11, 12, 4, 4, 7, 9, 6, 9, 12, 13, 15, 12, 10, 10, 6, 6, 7, 12, 11, 
          15, 5, 11, 8, 11, 7, 11, 12, 14, 12, 8, 11, 9, 10, 6, 14, 14, 8, 4, 7, 10, 3, 14, 10, 17, 7,
          16, 9, 12, 11, 7, 11, 5, 11, 13, 9, 7, 9, 7, 11, 12, 13, 6, 9, 10, 13, 8, 18, 6, 16, 8, 4, 16, 
          8, 9, 5, 7, 9, 10, 11, 13, 12, 9, 11, 7, 9, 6, 7, 6, 11, 8, 5)

```

We will run 4 chains with 5000 iterations each and discard the first 50% of each as burn-in. 

```{r}
chains <- 4

samples <- lapply(1:chains, function(i) {
  x <- gibbs(data = data, 
        alpha = alpha, 
        beta = beta,
        inits = "random",
        n = 5000,
        burn_in = 0.5, 
        keep = 1)
  
  x %>% 
    cbind(chain = i)
    
}) %>% 
  do.call(rbind, .)

```


Posterior distributions are concentrated in small areas of space.  

```{r out.width=c('50%', '50%'), fig.show='hold', echo = FALSE}

p_lambda <- samples %>% 
  ggplot(aes(x = lambda, group = chain, color = as.factor(chain))) + 
  stat_density(geom = "line", position = "identity") + 
  labs(x = "Value", y = "Density", title = "Lambda") +
  guides(color = guide_legend(title = "Chain"))


p_mu <- samples %>% 
  ggplot(aes(x = mu1, y = mu2, color = as.factor(chain))) + 
  geom_point(alpha = .25) +
  labs(title = expression(mu~" posterior"), x = expression(mu["1"]), y = expression(mu["2"])) + 
  guides(fill = guide_legend("Density"), color = guide_legend(title = "Chain"))
  

# p <- plot_grid(p_lambda, p_mu, ncol = 1)

p_lambda
p_mu

```


Data with inferred change point and latent Poisson process rates (posterior mode estimates with 50% credibility intervals): 

```{r}

MAP_estimates <- lapply(1:3, function(i) {
  d <- density(samples[, i])
  
  d$x[which.max(d$y)]
  
}) %>% unlist %>% set_names(c("lambda", "mu1", "mu2"))
  


p <- data.frame(y = data, x = 1:length(data)) %>% 
  ggplot() + 
  geom_point(aes(y = y, x = x)) + 
  geom_vline(xintercept = MAP_estimates["lambda"], color = "red") + 
  geom_line(data = data.frame(x = c(0, MAP_estimates["lambda"]), y = c(MAP_estimates["mu1"], MAP_estimates["mu1"])), aes(x = x, y = y), color = "blue", linetype = "dashed", size = 1.5) + 
  geom_line(data = data.frame(x = c(MAP_estimates["lambda"], N), y = c(MAP_estimates["mu2"], MAP_estimates["mu2"])), aes(x = x, y = y), color = "blue", linetype = "dashed", size = 1.5) 
  

print(p)

```


## Convergence 

The trace plots show the the chains seem well mixed. 

```{r}

p_trace <- samples %>%
  cbind(iter = 1:nrow(samples)) %>% 
  melt(id.vars = c("chain", "iter")) %>%
  ggplot(aes(x = value, color = as.factor(chain))) +
  geom_line(aes(x = iter, y = value)) + 
  facet_wrap(~variable, scales = "free", ncol = 1) + 
  guides(color = guide_legend(title = "Chain"))

print(p_trace)

```




Another diagnostic tool for MCMC samplers is the Gelman-Rubin convergence diagnostic, often denoted as $\hat{R}$. It measures the stability of estimates between different chains for each model parameter. Ideally the statistic should be close to 1 and often values larger than 1.1 are considered a sign of poor sampler performance. 

* $\hat{R}$ implemntation

```{r}
Rhat <- function(X) {
  
  n <- nrow(X)
  m <- ncol(X)
  
  # per chain variance
  chain_var <- lapply(1:ncol(X), function(i) var(X[, i])) %>% unlist
  
  # average per chain variance
  W <- mean(chain_var)
  
  # between chain variance
  th <- sum(colMeans(X))/m
  B <- (n/(m - 1))*sum((colMeans(X) - th)^2)
  
  # estimated variance of the stationary distribution
  stationary_var <- (1 - 1/n)*W + (1/n)*B
  
  # R hat statistic
  rhat <- sqrt(stationary_var / W)
  
  return(rhat)
  
}
```



$\hat{R}$ diagnostics for the inference are essentially 1 which yields extra validation that our implementation is indeed correctly built. 

```{r, echo = FALSE}
rhats <- lapply(c("lambda", "mu1", "mu2"), function(var) {
  
  df <- samples %>% 
    select(var, chain)
  
  per_chain_df <- lapply(unique(df$chain), function(i) {
    
    df %>% filter(chain == i) %>% 
      pull(var)
    
  }) %>% do.call(cbind, .)
  
  
  Rhat(per_chain_df)
  
}) %>% set_names(c("lambda", "mu1", "mu2"))


rhats
```








